{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "050367b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " קודי השגיאה השכיחים ביותר: [('404', 200094), ('400', 200068)]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "import heapq\n",
    "\n",
    "def split_file(log_file_path, chunk_size):#מחלק את הקובץ לחלקחם של 1000\n",
    "    df = pd.read_excel(log_file_path)  \n",
    "    chunks = [df.iloc[i:i + chunk_size] for i in range(0, len(df), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "def count_errors(df_chunk,error_dict):#סופר את כמות השגיאות לכל מספר שגיאה\n",
    "\n",
    "    for index, row in df_chunk.iterrows():\n",
    "        log_message = str(row[0])  \n",
    "        error_match = re.search(r\"ERR_(\\d+)\", log_message)\n",
    "\n",
    "        if error_match:\n",
    "            error_code = error_match.group(1)    \n",
    "            error_dict[error_code] = error_dict.get(error_code, 0) + 1   \n",
    "\n",
    "def get_top_n_errors(error_dict, N):#שימוש ב heap לצורך הוצאת כל ה n שגיאות המקסימליות \n",
    "    return heapq.nlargest(N, error_dict.items(), key=lambda x: x[1])\n",
    "#נכון להשתמש בערימה במקרה כזה כי אני לא צריכה מיון של כל הערימה אלא רק הוצאת n מספרים מקסימלים זמן ריצה nlogk\n",
    "\n",
    "def process_logs(log_file_path, chunk_size, N):\n",
    "    chunks = split_file(log_file_path, chunk_size)\n",
    "    error_dict = {}\n",
    "    [count_errors(chunk,error_dict) for chunk in chunks]\n",
    "    return get_top_n_errors(error_dict, N)\n",
    "\n",
    "log_file_path = r\"logs.txt.xlsx\"  \n",
    "chunk_size = 1000   \n",
    "N = 2  \n",
    "\n",
    "result = process_logs(log_file_path, chunk_size, N)\n",
    "print(\" קודי השגיאה השכיחים ביותר:\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ecf9275b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing: time_series.xlsx\n",
      " The final hourly averages have been saved to 'final_hourly_avg.csv'.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "def read_file(file_path):#קורא את הקובץ לכל סוגי הקבצים\n",
    "    ext = os.path.splitext(file_path)[1]\n",
    "    if ext == '.csv':\n",
    "        return pd.read_csv(file_path)\n",
    "    elif ext == '.parquet':\n",
    "        return pd.read_parquet(file_path)\n",
    "    elif ext in ['.xls', '.xlsx']:\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def clean_data(df):#מתקן את הדאטא משגיאות \n",
    "    # בדיקה שהעמודות קיימות\n",
    "    required_columns = {'timestamp', 'value'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "    \n",
    "    # המרת timestamp לפורמט תקני\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "\n",
    "    # המרת value לערך מספרי (יטפל בטקסטים כמו 'not_a_number')\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "\n",
    "    # הסרת נתונים עם ערכים חסרים או לא תקפים\n",
    "    df = df.dropna(subset=['timestamp', 'value'])\n",
    "\n",
    "    # הסרת כפילויות\n",
    "    df = df.drop_duplicates()\n",
    "\n",
    "    return df\n",
    "\n",
    "def split_file_by_day(df):#מחלק את הדאטא לחלקים לפי ימים\n",
    "    df['date'] = df['timestamp'].dt.date\n",
    "    chunks = [group.drop(columns='date') for _, group in df.groupby('date')]\n",
    "    return chunks\n",
    "\n",
    "def calculate_hourly_avg_per_chunk(chunks):#מחשב ממוצע שעתי\n",
    "    hourly_avgs = []\n",
    "    for chunk in chunks:\n",
    "        chunk['hour'] = chunk['timestamp'].dt.floor('H')\n",
    "        avg = chunk.groupby('hour')['value'].mean().reset_index()\n",
    "        avg.columns = ['timestamp', 'average_value']\n",
    "        hourly_avgs.append(avg)\n",
    "    return hourly_avgs\n",
    "\n",
    "def combine_hourly_avgs(hourly_avgs):#מצרף את כל הממוצעים לכל חלקי הדאטא\n",
    "    final_df = pd.concat(hourly_avgs)\n",
    "    final_df = final_df.sort_values('timestamp').reset_index(drop=True)\n",
    "    return final_df\n",
    "\n",
    "def process_log_file(log_file_path):\n",
    "    print(f\"Processing: {log_file_path}\")\n",
    "    \n",
    "    df = read_file(log_file_path)\n",
    "    df = clean_data(df)\n",
    "    chunks = split_file_by_day(df)\n",
    "    hourly_avgs = calculate_hourly_avg_per_chunk(chunks)\n",
    "    final_df = combine_hourly_avgs(hourly_avgs)\n",
    "\n",
    "    # שמירה לקובץ\n",
    "    output_path = 'final_hourly_avg.csv'\n",
    "    final_df.to_csv(output_path, index=False)\n",
    "    print(f\" The final hourly averages have been saved to '{output_path}'.\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    process_log_file('time_series.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2e3eeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "class StreamingHourlyAverages:#מחלקה לעיבוד זרימת נתונים בזמן אמת\n",
    "    def __init__(self):\n",
    "        self.data = defaultdict(list)\n",
    "\n",
    "    def add_entry(self, timestamp, value):#מוסיף נתון לממוצע\n",
    "        hour = pd.to_datetime(timestamp).floor('H')\n",
    "        self.data[hour].append(value)\n",
    "\n",
    "    def get_averages(self):#מוציא ממוצע עד כה\n",
    "        return {hour: sum(values) / len(values) for hour, values in self.data.items()}\n",
    "    \n",
    "def read_file(file_path):\n",
    "    ext = os.path.splitext(file_path)[1]\n",
    "    if ext == '.csv':\n",
    "        return pd.read_csv(file_path)\n",
    "    elif ext == '.parquet':\n",
    "        return pd.read_parquet(file_path)\n",
    "    elif ext in ['.xls', '.xlsx']:\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format\")\n",
    "\n",
    "def clean_data(df):\n",
    "    required_columns = {'timestamp', 'value'}\n",
    "    if not required_columns.issubset(df.columns):\n",
    "        raise ValueError(f\"Missing required columns: {required_columns - set(df.columns)}\")\n",
    "    \n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], errors='coerce')\n",
    "    df['value'] = pd.to_numeric(df['value'], errors='coerce')\n",
    "    df = df.dropna(subset=['timestamp', 'value'])\n",
    "    df = df.drop_duplicates()\n",
    "    return df\n",
    "\n",
    "file_path = \"time_series.xlsx\" \n",
    "df = read_file(file_path)\n",
    "df = clean_data(df)\n",
    "\n",
    "stream_processor = StreamingHourlyAverages()\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    stream_processor.add_entry(row[\"timestamp\"], row[\"value\"])\n",
    "\n",
    "hourly_averages = stream_processor.get_averages()\n",
    "\n",
    "output_df = pd.DataFrame([\n",
    "    {\"timestamp\": hour, \"average\": avg}\n",
    "    for hour, avg in sorted(hourly_averages.items())\n",
    "])\n",
    "output_df.to_csv(\"hourly_averages.csv\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
